\section{Experiments}
In this section we will discuss the structure of the test executed and their result.

\subsection{Machine Specification}
We executed the following tests using this environment:

\begin{itemize}
    \item \textbf{Machine}:
    \begin{itemize}
        \item Type: HP OmniBook Ultra Flip 14
        \item CPU: Intel® Core™ Ultra 7 - 258V (4.8 GHz, 12 MB cache L3, 8 core, 8 thread)
        \item RAM: 32 GB LPDDR5x-8533 MT/s
    \end{itemize}
    \item \textbf{Software}:
    \begin{itemize}
        \item OS: Ubuntu 24.04.3 LTS (Noble Numbat) x86\_64
        \item Host: Windows Subsystem for Linux - Ubuntu (2.5.7.0)
        \item Kubernetes: minikube version: v1.36.0, configured with 4 cores and 16GB of RAM
    \end{itemize}
\end{itemize}

\subsection{Experiments Description}
The experiments aim to emulate different types of traffic and demonstrate the behaviour of the system. The possible experiments can be performed choosing a distribution, and then specifying the set of parameters for every state of a state machine. In other words, if the user selects 4 different rates for the distribution selected the load is composed of 4 circularly executed states with the rates specified. The duration of each state can be modified from the code.

For example, choosing a uniform distribution with all rates value such that min=max (to model the deterministic arrival process), selecting a test duration of 9 minutes with rates = ([1,1];[2,2];[3,3];[5,5]) the result is an arrival process that generates a load with 1 message/s, than 2 messages/s, 3 messages/s, and 5 messages/s for 1 minute each. At this point, 4 minutes have elapsed, so the arrival process starts again cyclically from 1 message and so on until the end of the test.

\subsection{Experiments guide}
In this section, we provide a guide for reproducing our experiments. First of all, it is necessary to clone the GitHub repository at \href{https://github.com/Berags/oris-predictive-autoscaler}{this link} (https://github.com/Berags/oris-predictive-autoscaler). To start the project is necessary, as a minimum dependency, to have installed \href{https://minikube.sigs.k8s.io/docs/}{Minikube}. After successfully installing Minikube, run the following command:

\begin{table}[!htb]
\centering
\begin{tabular}{c}
\begin{lstlisting}
minikube start
\end{lstlisting}
\end{tabular}
\end{table}

then, it is sufficient to position the terminal on the directory of the project where the .sh file named ./start.sh is located and execute it with:

\begin{table}[!htb]
\centering
\begin{tabular}{c}
\begin{lstlisting}
./start.sh
\end{lstlisting}
\end{tabular}
\end{table}
 
This contains all the instruction necessary to build and run our project (i.e. all the containers and components). Don't mind if it takes a while, it is perfectly normal due to the project complexity. At this point, as reported in \guillemotleft port-forward.sh\guillemotright \ you can monitor:
\begin{itemize}
  \item The RabbitMQ queue at \guillemotleft http://localhost:15672\guillemotright
  \item Prometheus at \guillemotleft http://localhost:9090\guillemotright
  \item Grafana (with all the dashboards of interest shown below) at \guillemotleft http://localhost:3000\guillemotright, the default username and password is \verb|admin|.
  \item Kafdrop (for the Kafka queue) at \guillemotleft http://localhost:9000\guillemotright
  \item It's possible to monitor the state of the Sirio Controller's JVM using \textit{jconsole} with the address \guillemotleft http://localhost:9000\guillemotright.
\end{itemize}

At this point everything is ready to start the tests. To do so, move to the folder \guillemotleft k6 \guillemotright and run:
\begin{table}[!htb]
\centering
\begin{tabular}{c}
\begin{lstlisting}
./build-and-run.sh
\end{lstlisting}
\end{tabular}
\end{table}

This command will show the following menu:
\begin{lstlisting}[mathescape]
1. Exponential
2. Poisson ($\lambda$<100)
3. Uniform (Use min = max for deterministic)
4. Erlang (k, $\lambda$)
5. Exit
----------------------------
Insert your choice:
\end{lstlisting}

Here you can choose the distribution of the load you want to simulate. Then, all that remains is to select the required parameters as described above and as shown below. Finally, the test begins and the console displays some logs. To monitor the entire system, refer to the addresses indicated above. 

\subsubsection{Experiment personalization}

By cloning the repository, you can modify the code as you wish, but if you want to change the transition time, simply modify the following instruction in rabbitmq-test.js:

\begin{table}[!htb]
\centering
\begin{tabular}{c}
\begin{lstlisting}
const transitionTime = DistributionFactory.getDeterministic(60);
\end{lstlisting}
\end{tabular}
\end{table}

with \guillemotleft DistributionFactory.getDeterministic(60) \guillemotright \ defining the seconds of permanence in each state. If you are not interested in a deterministic transition, you can replace it, for example, with an exponential one with:

\begin{table}[!htb]
\centering
\begin{tabular}{c}
\begin{lstlisting}
const transitionTime = DistributionFactory.getExponential(20);
\end{lstlisting}
\end{tabular}
\end{table}


Please note that we have modelled the arrival processes of interest to us using the js library included in the code repository under k6/lib (all documentation included), but ultimately it is also possible to use the other supported distributions.

\subsubsection{Setup Details}

Here are reported more details on the usage of the \textit{.sh} file inside the main directory. Essentially, the fundamental ones are \guillemotleft start.sh\guillemotright \ and \guillemotleft build-and-run.sh\guillemotright \. Since ./start.sh does a lot of work, if something goes wrong is not necessary to re-deploy everything, so we provided \guillemotleft port-forward.sh\guillemotright \ for the port forwarding only and \guillemotleft delete\_and\_deploy\_sirio.sh\guillemotright \ to, as its name suggests, delete and redeploy only sirio. The \guillemotleft delete\_and\_cdf\_genereator.sh\guillemotright does the same, but for the \textit{inter-arrival-collector} service.

\subsection{Arrival Modeling}
For all the test, we modeled in Sirio the Arrival process as an Exponential with parameter $\lambda$ equal to the inverse of the average inter-arrival time of the messages in the queue (or $\lambda$ equals to the average messages per second, that is equivalent). 

We also tried using Bernstein phase types, but encountered a problem. It appears that the Sirio model of a Bernstein phase type uses a rapidly increasing amount of RAM as the number of phases increases. For example, we tried with 50 phases and the scaler attempted to instantiate more than 768 MB in the heap, causing the pods to overflow their memory and be terminated by Kubernetes. With fewer phases, we did not get a sufficiently good approximation to use in the scaler. On the other hand, trying to extend the amount of RAM available to the Sirio pod caused the machine's operating system to hang most of the time.

Furthermore, we found that the exponential approximation is more than adequate for a uniform arrival rate. Furthermore, having a microservice controller that consumes so many resources seems strange and counterproductive, as well as contrary to the very principles of microservices.

\subsection{Rejection Rate Calculation}
Here we want to discuss how we calculated the rejection rate, and the reasons for it. For our experiments, we wanted to calculate a relative rejection rate. In fact, we consider this approach more robust if compared to an absolute target.

The rejection rate, at least in terms of expected value, can be defined as: \textit{the probability that a packed get rejected by the queue}. To verify this condition, two pre-conditions must be true:
\begin{itemize}
    \item The Queue is full.
    \item The arrival process is habilitated to push a new message.
    \item The arrival process extracts a time to fire smaller than the service process.
\end{itemize}

Considering a stochastic model expressed in term of a Stochastic Time Petri Net (STPN), we are interested in only three elements: the place of the queue, the last transition of the arrival process that goes into the queue, and the first transition of the service that pulls from the queue (most of the time the service is represented with only a transition). For this problem we assume that the stochastic properties of the model depends only on the marking state. So, let $R$ be the event of a rejection, $Q=q_{max}$ the event of a marking with the queue full, $A$ the event of a marking where the last arrival transition has a token as a precondition, $t_a$ and $t_s$ the values sampled from the arrival and service transitions, from the previous conditions we can derive the equation \ref{eq:rejection}:

\begin{equation}
    \label{eq:rejection}
    \begin{split}        
    P(R)& =P(Q=q_{max} \land A\land t_a<t_s)\\
    & = P(t_a<t_s|Q=q_{max} \land A)P(Q=q_{max} \land A)
    \end{split}
\end{equation}

Considering that we used a Markovian model, the arrival and service transitions samples their times to fire form exponential transitions. So, let $\lambda_a$ and $\lambda_s$ of two transition when the queue is full and the arrival process able to push a new token into it, we can rewrite equation \ref{eq:rejection} as \ref{eq:rejection_exp}:
\begin{equation}
    \label{eq:rejection_exp}
    \begin{split}
    P(R) & =  P(t_a<t_s|Q=q_{max}\land A)P(Q=q_{max}\land A) \\
     & = P(Exp(\lambda_a)<Exp(\lambda_s))P(Q=q_{max}\land A)
    \end{split}
\end{equation}

Can be easily derived the probability of an exponential samples before another as in equation \ref{eq:first_exp}, so that the final rejection rate formula is \ref{eq:rejection_fin}.
\begin{gather}
P(Exp(\lambda_a)<Exp(\lambda_s)) = \frac{\lambda_a}{\lambda_a + \lambda_s} \label{eq:first_exp}\\
P(R)=\frac{\lambda_a}{\lambda_a + \lambda_s}P(Q=q_{max}\land A) \label{eq:rejection_fin}
\end{gather}

Considering that, given a closed model, Sirio is able to enumerate all reachable states, using steady-state analysis we can obtain the probability of having a state in which the queue is full and the arrival transition is enabled, while using \textit{MarkingExpresions} to obtain transition rates is trivial.

To highlight how this formula can be translated in practice, it is reported the code that calculated the rejection rate.
\begin{lstlisting}
BigDecimal rejection = BigDecimal.ZERO;
Map<Marking, BigDecimal> results = RegSteadyState.builder().build().compute(pn, m).getSteadyState();
for (Marking tmp : results.keySet()) {
    if (tmp.getTokens(queuePlace) == queue.getSize() && pn.isEnabled(arrivalTransition, tmp)) {
        BigDecimal currentRejection = results.get(tmp);

        BigDecimal arrivalRate = extractLambda(arrivalTransition, tmp).setScale(8, RoundingMode.HALF_UP);
        BigDecimal serviceRate = extractLambda(serviceTransition, tmp).setScale(8, RoundingMode.HALF_UP);

        currentRejection = currentRejection.multiply(arrivalRate).divide(arrivalRate.add(serviceRate), 8, RoundingMode.HALF_DOWN);

        rejection = rejection.add(currentRejection);
    }
}
return rejection;
\end{lstlisting}

\subsection{Experiments Setup}
In all tests we set some common parameters:
\begin{enumerate}
    \item Rejection Rate: 5\%.
    \item Service Rate: 1.
    \item Duration of states in the arrival process: 60s.
    \item Total test duration: 20 minutes.
\end{enumerate}

For the experiments with constant arrival time, we defined a state machine with the following parameters: [0.33,0.33],[0.2,0.2],[0.11,0.11],[1,1],[0.14,0.14]. In this, for every state we sample the same inter arrival time with the following expected messages per second: 3, 5, 9, 1 and 7. Considering that the state machine is cyclic, this behavior repeats periodically.

Then, we tried a stochastic workload using exponential as inter arrival time distributions in the machine states. The parameters used are: 3, 5, 7, 1, 9. These are also the expected messages per second of the respectively state.

In this report, we separated the logic of establishing the optimal number of replicas (the Recommender) from the one that communicates with Kubernetes (Updater). This allows to apply different scaling logics to the workers. In the following we will confront an immediate application of the recommendation, versus a sliding window approach that waits a series of downscale recommendation before applying it.

\subsection{Results}
\subsubsection{Immediate Apply of Recommendations}

In executing this experiment, both for constant and exponential arrival rates, we measure the arrived and rejected messages in figures \ref{fig:default_constant_messages} and \ref{fig:default_exponential_messages}.

\begin{figure}[H]
	\begin{subfigure}{0.49\linewidth}
	    \centering
	    \includegraphics[width=1\linewidth]{images/default/constant/messages.png}
	    \caption{Arrived and Rejected messages per second in constant rates.}
	    \label{fig:default_constant_messages}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
	    \centering
    		\includegraphics[width=1\linewidth]{images/default/exponential/messages.png}
	    \caption{Arrived and Rejected messages per second with exponential arrival times.}
    		\label{fig:default_exponential_messages}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
	    \centering
	    \includegraphics[width=1\linewidth]{images/default/constant/rejection_cumulative.png}
	    \caption{Cumulative Rejection rate for constant arrival time experiment.}
	    \label{fig:default_constant_rejection}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
	    \centering
	    \includegraphics[width=1\linewidth]{images/default/exponential/rejection_cumulative.png}
	    \caption{Cumulative Rejection rate for exponential arrival time experiment.}
	    \label{fig:default_exponential_rejection}
	\end{subfigure}
	\caption{Results of tests with immediate update.}
	\label{fig:default}
\end{figure}

As said, the goal of the control is to keep a rejection rate below 5\%. For both of them, rejection rate was under the threshold, with even the exponential without rejected messages at all.

We wanted to see not only if the target was achieved, but also the evolution in the number of recommended pods. The trend of the scaling can be seen in figures \ref{fig:default_constant_pods} and \ref{fig:default_exponential_pods}, with the respective lag cost in figures \ref{fig:default_constant_lag} and \ref{fig:default_exponential_lag}. More detail on how the costs are calculated can be found in section \ref{sec:lag_cost}.

\begin{figure}[H]
	\begin{subfigure}{0.49\linewidth}
	    \centering
	    \includegraphics[width=1\linewidth]{images/default/constant/pods.png}
	    \caption{Number of pods trend for constant arrival time experiment.}
	    \label{fig:default_constant_pods}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
	    \centering
    		\includegraphics[width=1\linewidth]{images/default/exponential/pods.png}
	    \caption{Number of pods trend for exponential arrival time experiment.}
    		\label{fig:default_exponential_pods}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
	    \centering
	    \includegraphics[width=1\linewidth]{images/default/constant/lag_cost_cumulative.png}
	    \caption{Cumulative lag cost for constant arrival time experiment.}
	    \label{fig:default_constant_lag}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
	    \centering
	    \includegraphics[width=1\linewidth]{images/default/exponential/lag_cost_cumulative.png}
	    \caption{Cumulative lag cost for exponential arrival time experiment.}
	    \label{fig:default_exponential_lag}
	\end{subfigure}
	\caption{Measure of pod instances with immediate update.}
\end{figure}

The surge of lag cost came mainly from downscaling. In fact, when Sirio Scaler recommends increasing the number of pods, Kubernetes immediately starts creating them to accommodate the new incoming traffic. On the other hand, when Sirio recommends a reduction, the excess pods cannot be terminated instantly, mainly because they are still processing some items.

Another issue that increases lag costs is that pods can be much higher than recommended. This happens when recommendations decrease and increase in a short period of time. In fact, the first decrease causes most pods to be terminated, but when the second increase comes, since terminated pods cannot be recovered, new ones must be created to compensate.

A summary of this results can be seen in table \ref{tab:default_summary}.

\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		& Constant & Exponential \\
		\hline
		Recommended Cost & 6030 & 2880 \\
		\hline
		Effective Cost & 8180 & 6835 \\
		\hline
		Relative Lag & 35.66\% & 137.33\%  \\
		\hline
		Rejection Rate & 0.58\% & 0.00\% \\
		\hline
	\end{tabular}
	\caption{Summary of the results for the immediate application of recommendations.}
	\label{tab:default_summary}
\end{table}

\subsubsection{Sliding Window}
In our experimenting we also tired to implement an updater that uses a sliding window to reduce the lag cost. In the case of a upscaling recommendation that will be immediately implemented, in the case of a downscaling the system maintains some inertia before doing it.

More precisely, for every downscaling the recommended value is inserted into a history. When the history reaches a certain dimension, and it has values smaller of the current value, the target is updated with the maximum in the history. Every time the target is changed the history is cleared.

For more details, below there is the snipped of code that implements this behavior.

\begin{lstlisting}
int currentReplicas = getCurrentReplicas();

if (newReplicas > currentReplicas) {
	replicaHistory.clear(); 
	return newReplicas;
} else if (newReplicas < currentReplicas) {
	replicaHistory.add(newReplicas);
	if (replicaHistory.size() > scalingDecisionWindow) {
		replicaHistory.removeFirst();	
	}

	if (replicaHistory.size() >= scalingDecisionWindow
         && replicaHistory.stream().allMatch(r -> r <= currentReplicas)) {                
		int maxInHistory = replicaHistory.stream().max(Integer::compareTo).orElse(newReplicas);
		replicaHistory.clear();
		return maxInHistory;
	}
} else {
	replicaHistory.add(newReplicas);
	if (replicaHistory.size() > scalingDecisionWindow) {
		replicaHistory.removeFirst();	
	}
}
return currentReplicas;
\end{lstlisting}

As in the precedent couple of test, in the figure \ref{fig:sliding_window} we report the trend of arrived messages confronted with the cumulative rejection rate.

\begin{figure}[H]
	\begin{subfigure}{0.49\linewidth}
	    \centering
	    \includegraphics[width=1\linewidth]{images/sliding_window/constant/messages.png}
	    \caption{Arrived and Rejected messages per second in constant rates.}
	    \label{fig:sliding_window_constant_messages}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
	    \centering
    		\includegraphics[width=1\linewidth]{images/sliding_window/exponential/messages.png}
	    \caption{Arrived and Rejected messages per second with exponential arrival times.}
    		\label{fig:sliding_window_exponential_messages}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
	    \centering
	    \includegraphics[width=1\linewidth]{images/sliding_window/constant/rejection_cumulative.png}
	    \caption{Cumulative Rejection rate for constant arrival time experiment.}
	    \label{fig:sliding_window_constant_rejection}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
	    \centering
	    \includegraphics[width=1\linewidth]{images/sliding_window/exponential/rejection_cumulative.png}
	    \caption{Cumulative Rejection rate for exponential arrival time experiment.}
	    \label{fig:sliding_window_exponential_rejection}
	\end{subfigure}
	\caption{Results of tests using the sliding window.}
	\label{fig:sliding_window}
\end{figure}

In the same way, the following figures shows the recommended pods with the associated cumulative lag cost.

\begin{figure}[H]
	\begin{subfigure}{0.49\linewidth}
	    \centering
	    \includegraphics[width=1\linewidth]{images/sliding_window/constant/pods.png}
	    \caption{Number of pods trend for constant arrival time experiment.}
	    \label{fig:sliding_window_constant_pods}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
	    \centering
    		\includegraphics[width=1\linewidth]{images/sliding_window/exponential/pods.png}
	    \caption{Number of pods trend for exponential arrival time experiment.}
    		\label{fig:sliding_window_exponential_pods}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
	    \centering
	    \includegraphics[width=1\linewidth]{images/sliding_window/constant/lag_cost_cumulative.png}
	    \caption{Cumulative lag cost for constant arrival time experiment.}
	    \label{fig:sliding_window_constant_lag}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
	    \centering
	    \includegraphics[width=1\linewidth]{images/sliding_window/exponential/lag_cost_cumulative.png}
	    \caption{Cumulative lag cost for exponential arrival time experiment.}
	    \label{fig:sliding_window_exponential_lag}
	\end{subfigure}
	\caption{Measure of pod instances using the sliding window.}
\end{figure}

Lastly, table \ref{tab:sliding_window_summary} report a summary of experiments using the sliding window approach.

\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		& Constant & Exponential \\
		\hline
		Recommended Cost & 4725 & 5420 \\
		\hline
		Effective Cost & 5130 & 6940 \\
		\hline
		Relative Lag & 8.57\% & 28.04\%  \\
		\hline
		Rejection Rate & 0.36\% & 0.00\% \\
		\hline
	\end{tabular}
	\caption{Summary of the results for the sliding window approach.}
	\label{tab:sliding_window_summary}
\end{table}

\subsubsection{Comments}
From the above result many thing can be observed. Before commenting them, it's important to notice that the total amount of messages generated in every case is mostly the same.

The first result that can be noted is that the introduction of a sliding window significantly reduces the cost introduced by the Kubernetes delay. However, this reduction is mainly due to the increase in the recommended cost. In fact, in the case of a constant arrival time, we obtained a lower total cost, but in the exponential case, there was no significant change (in fact, there was a slight increase, but this can be explained by variations between one execution and another).

Certainly, the sliding window strategy has made the number of pods more stable over time, avoiding excessive use of resources. To better frame this concept, in previous experiments conducted in a different context and on a different machine, the exponential case without a sliding window always caused a crash, while its use allowed for smooth termination.

Another interesting trend that can be seen in this data is that, for some reason, the constant inter arrival time seems harder to manage than the exponential. In both cases the constant arrival rate is characterised by some rejected messages, while the exponential does not. We cannot say whether this phenomenon happened by chance or whether there is some hidden reason behind it.

Finally, something can be said about the very low recommended numerical cost achieved (in theory) in the exponential case without a sliding window. We can see how it is very common to see it reach 1 (the minimum). Since both the arrival and service processes are stochastic, it may happen that the queue empties. This makes the CDF generator component of the system fail to generate valid CDF. If Sirio doesn't receive any messages for a give time (10s as our default choice) it simply \textit{thinks} that there is no load, and recommend for the minimum amount of pods (1 in this case). It's important to notice that terminating pods are probably still elaborating at least the last messages that they were able to pull, factually discharging the queue. These two phenomena combined result in very low recommended costs, but still a zero rejection rate. The actual cost metric reflects this behaviour.

